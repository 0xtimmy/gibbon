
% This varies by conference, sometimes 9pt:
\documentclass[preprint,10pt,nocopyrightspace,nonatbib]{./bibs/sigplanconf}
% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.


%% Common packages we use:

%% This is our standard package for code formatting:
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{graphicx}
% \usepackage{mathabx}
% \usepackage{mathpartir}

\usepackage[noabbrev]{cleveref}
\usepackage{enumitem}

\usepackage{wrapfig}

\newcommand{\gramdef}{\; ::= \;}
\newcommand{\gramor}{\; | \;}
\newcommand{\keywd}[1]{\; \texttt{#1} \;}
\newcommand{\keywdr}[1]{\; \texttt{#1}^{*} \;}

\newif\ifcurly
\curlytrue % Comment to deactivate.


%----------------------------------------
%% \usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
%% \usepackage{xargs}
%% \newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
%% \newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
%% \newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
%% \newcommandx{\inconsistent}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=red,#1]{#2}}
%% \newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
%% \newcommandx{\resolved}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}} % use this to mark a resolved question
%% \newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}} % will replace \resolved in the final document
% ----------------------------------------


% Tweak width of margin notes for this documentclass:
\setlength{\marginparwidth}{1.75cm}
% Copy this if needed to customize:
\input{./bibs/latex_templates/editingmarks}

% If we are using Haskell code in this paper:
\input{./bibs/latex_templates/haskell_style}

% Sometimes we have extra short-cuts:
% \input{macro-defs}

% Sometimes we factor large figures into commands in a separate file:
% \input{figures}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country}
\copyrightyear{20yy}
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm}
\copyrightdoi{nnnnnnn.nnnnnnn}

% Uncomment the publication rights you want to use.
%\publicationrights{transferred}
%\publicationrights{licensed}     % this is the default
%\publicationrights{author-pays}

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\newcommand{\treelang}{TreeLang} % NEED NAME

\title{Compiling tree transforms to operate on packed representations}
% \subtitle{Faster compiler passes, parallelization ready}

\authorinfo{Name1}
           {Affiliation1}
           {Email1}
\authorinfo{Name2\and Name3}
           {Affiliation2/3}
           {Email2/3}

\maketitle

\begin{abstract}
When written idiomatically in most programming languages, programs that traverse
and construct trees operate over pointer-based data structures, with one heap
object per-leaf and node.  While this may seem tautological, we show that common
tree traversals---found in compiler passes, space-partitioning trees, and
elsewhere---can instead be automatically compiled to operate on pointer-free
pre-order serializations of trees.  On current x86 architectures such programs
can run up to several times faster than their pointer-based counterparts.

We present a prototype compiler for a small first-order, purely functional
language of tree traversals.  The output language includes mutable cursors into
input and output buffers for packed data.  We propose a compilation technique
with an effect system for capturing traversal behavior, combined with a
lightweight analysis inferring data flow, and a program synthesis step for
creating missing traversals.
  
\end{abstract}

\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore,
% you may leave them out
\terms
term1, term2

\keywords
keyword1, keyword2

% ================================================================================
\section{Introduction}
% ================================================================================

\rn{This is an example peanut-gallery comment.}

Programs that traverse and construct trees are a niche, but an important one,
including compiler passes, HTML DOM, and particle simulations with
space-partitioning trees.
%
Yet there is almost no diversity in how modern programming languages and
compilers represent trees and their traversals, so the matter would seem to be
settled.
% In the traditional representation,
Each node of the tree is a heap object, followed by fields for child nodes or
leaf values.  This representation hasn't changed since the dawn of computing and
is shared across source languages with quite different type
systems---whether algebraic datatypes or class hierarchies, either statically or
dynamically typed.  The only deviations from this consensus are found within
limited high-performance scenarios where complete trees can be laid out using
address arithmetic with no intermediate nodes \cite{hpc-trees}.

But perhaps the consensus was premature?  In numerical computing it is an axiom
that you cannot treat the numbers in a matrix as individual heap objects.
Rather, the emphasis is on bulk efficiency.  Likewise, many tree traversals
process trees in bulk, reading or writing them in one pass.  On such workloads,
traditional tree representations are not favored by current trends in computer
architecture.  Pointer chasing implies randomized memory access patterns.
%
While previous work addresses spatial locality for tree data\cite{tree-locality-pldi},
%
much memory is still wasted both in pointers themselves and in tags on nodes
(e.g. distinguishing ``interior'' vs ``leaf'' objects).  For example, a C
compiler uses 96 bytes
%
%% \rn{Double check that for alignment the C compiler is sacrificing a word for the
%% one byte tag field.} 
%
of memory to represent the following tree:  

% \begin{figure}
%% \begin{figure}
%% %\begin{wrapfigure}{r}{0.5\textwidth} 
%% \Red{PICTURE HERE:  possibly with wrapfig.}
\begin{verbatim}
 [N|.|.]          struct Tree {
    /  \           enum { Leaf, Node } tag;
[L 1]  [N|.|.]     union { 
         /   \      struct { long long elem; };
      [L 2] [L 3]   struct { struct Tree* l;
                             struct Tree* r; }}}
\end{verbatim}

On the other hand, if we were sending the tree over the network, we would
naturally use a more compact format in serializing it:

%%   \caption{Traditional, pointer-based tree layout.}
%% \end{figure}
%\end{wrapfigure}
\begin{verbatim}
  [N L 1  N L 2  L 3 ]
\end{verbatim}
% \end{figure}

Here we use the same 24 bytes for the leaf numbers, but only 5 bytes for the
spine, rather than 72.  Further, a tree traversal processing this memory
representation follows a precisely linear memory access pattern, because the
data is already laid out in a preorder traversal.
%
On architectures with inexpensive unaligned access, such as modern x86, this is
a desirable in-memory representation as well as a serialization format.

Indeed, if we can compile programs to operate directly on this serialization, we
follow a precedent of using serialization formats jointly as in memory formats.
For example, Cap'N Proto \cite{capnproto} makes it ergonomic for C++ code to operate directly
on the Protobuf serialization format in memory.  Likewise ``data baking''
\cite{data-baking} is an established practice in video games---caching assets on
disk in a format that allows them to be \il{mmap}'d into memory and used
without further conversion.  As a general example of this capability, the
Glasgow Haskell Compiler (GHC) recently added the capability to store any closed
subgraph of the heap as a {\em Compact Normal Form} (CNF) \cite{cnf-icfp15}---a
contiguous memory region that is treated as a kind of ``super object'', never
traced by the GC and collected only when there are no pointers into any of the
sub-parts of the CNF.

The packed tree format above is precisely a dense encoding of a CNF---a
transitive closure of heap objects with no escaping pointers, in this case, no
pointers {\em at all}.  But unlike GHC's CNF support, which uses regular heap
objects, colocated together, the dense tree format {\em requires a complete
rearrangement of the compiled code that operates on the data}.

% Shall we acronymize it?
% DTF - dense tree form, or dense tree format
% PTF - packed tree form
% DTP - dense tree packing

In this paper, we take a first step towards compiler support for transparently
packing tree datatypes {\em without} changing the source program.  We make the
following contributions:

\newcommand{\calculus}{$\lambda^D_T$} % uh, Dense Tree?

\begin{itemize}
\item We use a small core language, \calculus{}, to formalize a transformation
  on programs with threes that creates an output program that operates only on
  pointers in the serialized representation.  This involves a small program
  synthesis step, and we show how the program synthesis and the selection of
  data structure shape go benefit from co-optimization.
  
\item We demonstrate that a prototype language implementation based on this
  transformation can compile a range of tree transforms to be several times
  faster than existing techniques~\cref{sec:eval-compiler}.
  
\item We show that the principle of packed-tree traversals works across a
  variety of language implementations, by benchmarking handwritten packed-tree
  programs~\cref{sec:eval-shootout}.
  
\item We show that not only do tree traversals become faster, but that their
  {\em potential parallel speedup}, increases as well \cref{sec:eval-parallel}.
\end{itemize}


% ================================================================================
% \section{Motivating Example}
\section{Background and Example}
% ================================================================================

While our main emphasis in this paper is on accelerating compiler passes, here
we continue with our simple example from the introduction: binary trees with
integer leaves.
In a language with algebraic datatypes,
% In a strongly typed functional language,
a recursive walk on
the tree would typically use pattern matching:

\ifcurly
% [language=python]
\begin{code}[language=c]
type Tree = Leaf(Int) | Node(Tree,Tree);

fun add1(t) {
  match(t) {
    Leaf(n):   return Leaf(n+1);
    Node(x,y): return Node(add1(x),add1(y));
  }}
\end{code}
\else
\begin{code}
 data Tree = Leaf Int | Node Tree Tree
  
 add1 t =
  case t of
    Leaf n   -> Leaf (n+1)
    Node x y -> Node (add1 x) (add1 y)
\end{code}
\fi

In fact, the small, first-order, purely functional language of tree traversals
we consider this paper is already a subset of most existing languages.
The above program is not substantially different in C, Haskell, ML, Scala,
Swift, Rust, etc.  Only the details of switching on sum types (tagged unions) differ.

The first problem for tree-walks such as this is memory management.  \il{add1}
can easy become a malloc or garbage collector benchmark.  For instance, the
following C code is over twice as slow as the same implementation in Java or a
good functional compiler:

\begin{code}[language=c]
  Tree* add1(Tree* t) {
    Tree* tout = (Tree*)malloc(sizeof(Tree));
    tout->tag = t->tag;
    if (t->tag == Leaf) {
      tout->elem = t->elem + 1;
    } else {
      tout->l = add1(t->l);
      tout->r = add1(t->r);
    }
    return tout;
  }
\end{code}

But even if we assume optimal layout, bump-pointer allocation, and {\em no
  header objects}---even if we go further and enable \il{__packed__} attribute
for our structs to save tag space---the performance of the above code is still
several times below what is achievable.  The main observation of this paper is
that bulk tree walks are efficient if done directly on pre-order serialization
of the tree, and that it is possible to automate the translation of recursive
functions, such as @add1@ above, into code that directly manipulates data
buffers containing serialized trees.

For our simple example, this buffer passing code isn't complicated:
%
\begin{code}[language=c]
char* add1(char* tin, char* tout) {
  if (*tin == Leaf) {
    *tout = Leaf;
    tin++; tout++;
    *(int*)tout = *(int*)tin;
    return (tin + sizeof(int));
  } else {
    *tout = Node;
    tin++; tout++;
    char* t2 = add1Tree(tin,tout);
    tout += (t2 - t);
    return add1Tree(t2,tout);
  }
}
\end{code}
%
But it is tedious and error prone to write, and cannot scale.  Clearly, no one
would use a technique like this for building a compiler!

The above program is similar to the output produced by the prototype compiler we
describe in this paper.  We refer to the input and output pointers as {\em
  cursors}, and one of the primary jobs of the compiler is automatic cursor
insertion.

%% \note{Yet, code performing raw manipulation of a pointer into a byte buffer
%%   containing a serialized tree is extremely error prone and tedious to write.
%%   It certainly is inappropriate for writing compilers, which contain reams and
%%   reams of tree-walking code.}

\subsection{Challenges and Limitations}

\note{Rightmost vs leftmost}


\ifcurly
\begin{code}[language=c]
fun left(t) {
  match(t) {
    Leaf(n):   return n;
    Node(x,_): return left(x);
  }}
fun right(t) {
  match(t) {
    Leaf(n):   return n;
    Node(_,y): return right(y);
  }}
\end{code}
\else
\begin{code}
 left t = case t of
            Leaf n   -> n
            Node x _ -> left x
 right t = case t of
             Leaf n   -> n
             Node _ y -> right y
\end{code}
\fi


\subsection{Related Work}

\rn{Milind, some of the related work was going to go up here?}

One line of closely related work focuses on managing data layout in trees and
other data structures to promote spatial
locality~\cite{Chilimbi1999,Chilimbi1999b,Truong1998,Lattner2005,Chilimbi1999},
by modifying garbage collection to co-locate objects~\cite{Chilimbi1999a},
modifying memory allocators to proactively place objects with similar access
patterns together~\cite{Lattner2005,Chilimbi1999}, or to modify the internal
layout of objects to place hot fields near each other~\cite{Chilimbi1999b}.
These approaches attempt to ``pack'' data together, using various techniques,
into cache lines to improve spatial locality, and hence have some resemblance
to our packed representations, which gain some performance benefits from
packing tree data into a compact format that promotes spatial locality.

Perhaps the most closely related of these is Chilimbi et al.'s {\em cache
conscious structure layout}~\cite{Chilimbi1999}. They propose a
cache-conscious data placement scheme where, given a traversal function,
tree-structured data will be laid out in memory in a {\em clustered} manner:
nodes from small subtrees will be placed on single cache lines. By matching
the tree layout to a specified traversal order, spatial locality is improved
when the tree is traversed in that order. A key difference between our packed
representation and Chilimbi et al.'s work is that this work focused on object
layout, without changing the internal {\em representation} of the objects.
Leaving the object representation of tree nodes the same allows code that
manipulates the objects to remain the same, but incurs costs: there is no
opportunity to reduce the space or instruction overhead incurred by pointers
linking nodes in the tree (see Figure~\ref{fig:intro-fig}), as exploiting that
opportunity requires code transformation. Most of the aforementioned spatial
locality work makes the same tradeoff.

One exception is Chilimbi et al.'s work on {\em automatic structure
splitting}~\cite{Chilimbi1999b}, where objects are transformed into split
representations, allowing hot fields from multiple objects to be co-located on
a single cache line while those objects' cold fields are placed elsewhere.
Because this layout optimization changes the internal representation of the
object, Chilimbi et al. develop a compiler pass that automatically transforms
code to work with the split representation. The transformations for structure
splitting concern how to access object fields, and hence, unlike our work, do
not require deeper transformations to remove the pointer dereferences inherent
in traversing linked data structures.


% ================================================================================
\section{The \treelang{} Language}
% ================================================================================

\note{Informal overview of language}

To demonstrate the compilation technique we are presenting, we present
$\treelang{}$, a typed programming language that is simple enough to
present briefly in a paper, and featureful enough to express some
interesting programs, such as common compiler passes.

Programs consist of a series of data type declarations and function
declarations. Similar to most functional programming languages,
programmers may define \emph{algebraic data types}, and dispatch on
them with a \texttt{match} language form (called \texttt{case} or
\texttt{switch} in some languages). For example, a data type for peano
numbers would have two cases: zero and successor.

\note{We already gave an example of a program in TreeLang earlier...
  probably only need one, unless we're doing something different here}

\begin{code}[language=haskell]
type Nat = Zero | Suc(Nat);
\end{code}

For simplicity, $\treelang{}$ is a first-order language, so all
functions are defined at the top level.

\begin{code}[language=haskell]
fun trav(x) {
  match(x) {
    Zero: return 0;
    Suc(n): return 1 + trav(n);
}}
\end{code}

In addition to algebraic data types and various numerical types, $\treelang{}$ supports
general purpose persistent dictionaries, which are useful for implementing compiler
passes in a functional style.

\note{Show an example of using a dictionary? Or just give the types?}

\note{Describe the Racket front-end and how it works.}

% ================================================================================
\section{Formal Language}
% ================================================================================


\note{The calculus, plus the core lowering transforms in figures.}

We present two languages: L1, a simple purely functional, first-order programming
language with sum and product types; and L2, an imperative programming language
with mutable arrays and pointer arithmetic, as well as a type-and-effect system.

\subsection{L1: Source language}

%% \note{Do we have a problem if we just stick to the way sums and products are usually presented?
%%   Or do we need recursive types? Also, binary sums and products might make the switch form
%%   in L2 less interesting.}

\note{This is horribly outdated and needs to be rewritten.}

\begin{figure}
  \begin{displaymath}
    \keywd{v} \in \; Var, \:
    \keywd{n} \in \; Int, \: \keywd{p} \in \; Prim, \: \keywd{c} \in \; Tag
  \end{displaymath}
  \begin{displaymath}
    \begin{aligned}
      \keywd{prog} \gramdef & \keywdr{decl} \keywd{e} \\
      \keywd{decl} \gramdef & \keywd{v} \mapsto \lambda \keywd{v:t}. \keywd{e} \\
      \keywd{t} \gramdef & \keywd{int} \gramor \keywd{t} \rightarrow \keywd{t} \gramor \keywd{t} * \keywd{t}  \\
      \gramor & \keywd{t} + \keywd{t} \\
      \keywd{e} \gramdef & \keywd{v} \gramor \keywd{n} \gramor \keywd{e} \keywd{p} \keywd{e} \gramor \keywd{e} \keywd{e} \\
      \gramor & \keywd{(e,e)} \gramor \# 1 \keywd{e} \gramor \# 2 \keywd{e} \\
      \gramor & \keywd{let} \keywd{(v,t,e)} \keywd{e} \\
      \gramor & \keywd{case} \keywd{e} \keywd{of} \keywd{(e,e)} \\
      \gramor & \keywd{inl} \keywd{t} \keywd{e} \gramor \keywd{inr} \keywd{t} \keywd{e} \\
    \end{aligned}
  \end{displaymath}
  \caption{Grammar for source language}
\end{figure}

\subsection{L2: Target language}

\begin{figure}
\begin{displaymath}
  \begin{aligned}
    \keywd{x} \gramdef & \keywd{n} \gramor \keywd{v} \\
    \keywd{t} \gramdef & \keywd{int} \gramor \keywd{cursor} \gramor \keywd{t} \rightarrow \keywd{t} \gramor \keywd{t} * \keywd{t} \\
    \keywd{e} \gramdef & \keywd{x} \gramor \keywd{x} \keywd{p} \keywd{x} \gramor \keywd{v} \keywd{x} \\
    \gramor & \keywd{(x,x)} \gramor \# 1 \keywd{x} \gramor \# 2 \keywd{x} \\
    \gramor & \keywd{let} \keywd{(v,t,e)} \keywd{e} \\
    \gramor & \keywd{ifeq} \keywd{(x,x)} \keywd{e} \keywd{e} \\
    \gramor & \keywd{switch} \keywd{x} \keywdr{(n,v,e)} \\
    \gramor & \keywd{newbuff} \keywd{n} \\
    \gramor & \keywd{writetag} \keywd{v} \keywd{n} \\
    \gramor & \keywd{writeint} \keywd{v} \keywd{x} \\
    \gramor & \keywd{readtag} \keywd{v} \\
    \gramor & \keywd{readint} \keywd{v} 
  \end{aligned}
\end{displaymath}
\caption{Grammar for target language}
\end{figure}

% ================================================================================
% \section{A tree-packing compiler}
\section{Compilation Algorithms}
% ================================================================================

\note{While we've seen examples above of cursor-passing programs that operate on
serialized data, these programs are extremely tedious and error prone.}


\subsection{Inferring effects}

% \newcommand{\arr}[3]{\ensuremath{{#1} \xrightarrow{\overrightarrow{#2}} {#3}}}
\newcommand{\arr}[3]{\ensuremath{{#1} \xrightarrow{trav({#2})} {#3}}}
% $\xrightarrow[world]{hello}$

\note{We denote a function with traversal effects as \arr{A}{A}{B}. }


\subsection{Routing end-of-value witnesses}


\subsection{Output cursor insertion}



% ================================================================================
\subsection{Discussion}
% ================================================================================

\note{Purity is important to reordering for witness search.}

\note{Our interpreter depends on this fact by modeling cursors {\em as lists}.}


% ================================================================================
\section{Implementation}
% ================================================================================

\note{Current prototype implementation, status}



% ================================================================================
\section{Evaluation}
% ================================================================================

\note{This is organized as a series of different case studies.}

\subsection{Microbenchmarks}
% ----------------------------------------

\note{A ``language shootout'' of add1-tree benchmarks.}


\subsection{Compiler passes on realistic inputs}
% --------------------------------------------

\note{Racket core AST benchmarks}


\subsection{Typechecking benchmark}
% --------------------------------------------

\note{type check the simply-typed $\lambda$ calculus in \treelang{}}

\subsection{KD-tree}
% --------------------------------------------

\note{Laith's benchmark}


\subsection{Parallelism opportunity study}
% --------------------------------------------

\note{Report preliminary parallel microbenchmark results, showing the potential
  for future work here.}




% ================================================================================
\section{Future work}
% ================================================================================

\note{Data type factoring, storing leaves in a separate, dense, aligned vector.
This enables (1) vectorization of numeric operations, and (2) separating
out pointers that the GC must traverse.  This can prove essential for an
open-world implementation in a managed language.}


\note{A large space of parallelism design choices.}

\note{}


% ================================================================================
\appendix
\section{Appendix Title}
% ================================================================================

This is the text of the appendix, if you need one.

\acks

Acknowledgments, if needed.

% \bibliographystyle{abbrvnat}
\bibliographystyle{abbrv}

% If you can't commit to the submodule right this second, just copy
% this file to ./refs.bib :
\bibliography{bibs/refs}

% The bibliography should be embedded for final submission.
%% \begin{thebibliography}{}
%% \softraggedright
%% \bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
%% P. Q. Smith, and X. Y. Jones. ...reference text...
%% \end{thebibliography}


\end{document}


